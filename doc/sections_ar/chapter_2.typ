#import "../helpers.typ": flex_captions, ar, ar_std, en, en_std, en_clean, en_clean_std

= #ar([الفصل الثاني: الأسس النظرية])

#ar_std([
  في هذا الفصل، سوف نتعمق في الأسس النظرية للتقنيات المستخدمة في هذا المشروع. سنبدأ باستكشاف بنية نموذج الاستماع والانتباه والتهجئة (#en_std([LAS]))، الذي يعد حجر الأساس في نظام التعرف على الكلام لدينا.
])

#linebreak()

== #ar([بنية الاستماع والانتباه والتهجئة (#en_clean([LAS]))])

#ar_std([
  نموذج الاستماع والانتباه والتهجئة (#en_std([LAS])) هو بنية شبكة عصبية مصممة لتحويل الكلام المنطوق مباشرة إلى تسلسلات حروف. وهو نظام شامل يتعلم جميع مكونات أداة التعرف على الكلام بشكل مشترك، بعيدًا عن نماذج #en_std([DNN-HMM]) التقليدية التي يتم تدريبها بشكل منفصل.
])

#linebreak()

#ar_std([
  جوهر #en_std([LAS]) هو إطار عمل *سلسلة إلى سلسلة* يتكون من وحدتين أساسيتين: وحدة تشفير تسمى *المستمع* ووحدة فك تشفير تسمى *المهجئ*. يقوم المستمع بمعالجة الصوت المدخل، ويقوم المهجئ بإنشاء النص المقابل،حرفًا بحرف.
])

#linebreak()

#ar_std([
  يسعى النموذج إلى ربط تسلسل مدخلات طيف بنك المرشحات، #en_std([$x = (x_1, ..., x_T)$]), بتسلسل مخرجات الأحرف، #en_std([$y = (y_1, ..., y_S)$]). ويتم تحقيق ذلك من خلال نمذجة الاحتمال الشرطي باستخدام قاعدة التسلسل:
])

#align(center)[
  #en_std([
    $P(y|x) = product_i P(y_i|x, y_(<i))$
  ])
]

#linebreak()

#ar_std([
  تقوم الدالتان الرئيسيتان، #en_std([`Listen`]) و#en_std([`AttendAndSpell`])، بتحويل الإشارة المدخلة #en_std([$x$]) إلى تمثيل عالي المستوى #en_std([$h$]) ثم تنتجان التوزيع الاحتمالي النهائي على تسلسل الأحرف.
])

#align(center)[
  #en_std([
    $h = "Listen"(x)$\
    $P(y|x) = "AttendAndSpell"(h, y)$
  ])
]

#pagebreak()

#ar_std([
  الشكل التالي يوضح بنية نموذج #en_std([LAS]). ويظهر الهيكل الهرمي للمستمع وربط مخرجات المستمع بمدخلات المهجئ.
])

#figure(
  image("../media/LAS_Full-Architecture.png", height: 80%),
  kind: image,
  caption: flex_captions(
    ar_std([البنية الشاملة لنموذج الاستماع والمتابعة والتهجئة (#en_clean_std([LAS]))، والتي توضح التدفق من المستمع إلى المهجئ.]),
    ar_std([بنية #en_clean_std([LAS]) الكاملة])
  )
)

#pagebreak()

=== #ar([المستمع (المشفّر)])

#ar_std([
  يعمل *المستمع* كجهاز تشفير للنموذج الصوتي. وتتمثل مهمته الأساسية في تحويل الخصائص الصوتية منخفضة المستوى من إشارة الكلام المدخلة #en_std([$x$]) إلى تمثيل للخصائص عالية المستوى #en_std([$h = (h_1, ..., h_U)$]).

  يتمثل أحد التحديات الرئيسية في التعرف على الكلام في طول إشارة الإدخال، التي يمكن أن تصل إلى آلاف الخطوات الزمنية. ستواجه شبكة #en_std([RNN]) القياسية صعوبة في استخراج المعلومات ذات الصلة من تسلسل طويل كهذا. لمعالجة هذه المشكلة، يستخدم مستمع #en_std([LAS]) بنية متخصصة تسمى شبكة #en_std([*pyramidal Bidirectional Long Short-Term Memory* (pBLSTM)]).
])

#linebreak()

#ar_std([
  الغرض الرئيسي من #en_std([pBLSTM]) هو *تقليل الدقة الزمنية* لتسلسل الميزات، مما يجعل السلسلة #en_std([$h$]) أقصر من المدخلات #en_std([$x$]) (أي #en_std([$U < T$])). في كل طبقة متتالية من #en_std([pBLSTM])، يتم تقليل الدقة الزمنية بمعامل #en_std([$2$]).

  يتم تحقيق ذلك عن طريدمج المخرجات من الخطوات الزمنية المتتالية في طبقة واحدة قبل إدخالها كمدخلات إلى الطبقة التالية. يتم تعريف قاعدة التحديث في طبقة #en_std([pBLSTM]) في المستوى #en_std([$j$]) في الخطوة الزمنية #en_std([$i$]) على النحو التالي:
])

#align(center)[
  #en_std([
    $h_i^j = "pBLSTM"(h_(i-1)^j, [h_(2i)^(j-1), h_(2i+1)^(j-1)])$
  ])
]

#linebreak()

#ar_std([
  في تطبيق الورقة البحثية، يكدس المستمع ثلاث طبقات #en_std([pBLSTM]) فوق طبقة #en_std([BLSTM]) أولية، مما يقلل الدقة الزمنية النهائية بمعامل #en_std([$2^3 = 8$]). هذا التخفيض الكبير في الطول يساعد آلية الانتباه على التركيز بسهولة أكبر، ويقلل أيضًا من التعقيد الحسابي العام للنموذج.
])

#pagebreak()

#ar_std([
  الشكل الآتي يوضح الطبقات الهرمية في المستمع وعملية تخفيف الدقة الزمنية.
])

#figure(
  image("../media/LAS_Listener-Architecture.png", height: 40%),
  kind: image,
  caption: flex_captions(
    ar_std([بنية المشفر #en_clean_std([Listener]). يستخدم #en_clean_std([BLSTM]) هرمي لتقليل الدقة الزمنية لميزات الإدخال #en_clean_std([$x$]) تدريجياً، مما ينتج عنه تسلسل ميزات أقصر وعالي المستوى #en_clean_std([$h$]) لبرنامج #en_clean_std([Speller]).]),
    ar_std([بنية المستمع])
  )
)

#linebreak()

=== #ar([المُهجّئ (المُفكّك)])

#ar_std([
  *المدقق الإملائي* هو مفكك رموز *قائم على الانتباه* يتلقى تمثيل السمات عالية المستوى #en_std([$h$]) من المستمع ويقوم بإنشاء النص النهائي حرفًا حرفًا.

  في كل خطوة خرج #en_std([$i$])، يحسب المدقق الإملائي توزيع الاحتمالات على جميع الأحرف الممكنة، بناءً على خرج المستمع #en_std([$h$]) وجميع الأحرف التي تم إنشاؤها مسبقًا #en_std([$y_(<i)$]). تدار هذه العملية بواسطة شبكة عصبية متكررة (#en_std([RNN])) (على وجه التحديد، #en_std([LSTM]) ذات طبقتين) تحافظ على حالة المفكك #en_std([$s_i$]) وتستخدم متجه السياق $c_i$ المقدم من آلية الانتباه. 
])

#pagebreak()

#ar_std([
  العلاقة بين الحالة #en_std([$s_i$]) للمفكك، ومتجه السياق #en_std([$c_i$])، واحتمال الحرف النهائي هي كما يلي:
])

#align(center)[
  #en_std([
    $s_i = "RNN"(s_(i-1), y_(i-1), c_(i-1))$\
    $P(y_i|x, y_(<i)) = "CharacterDistribution"(s_i, c_i)$
  ])
]

#linebreak()

#ar_std([
  تقوم آلية الانتباه بتوليد متجه سياق #en_std([$c_i$]) في كل خطوة. يزود هذا المتجه أداة التهجئة بمعلومات مركزة من الأجزاء ذات الصلة من الصوت المدخل اللازمة لتوليد الحرف التالي.
])

#linebreak()

#ar_std([
  تعمل الآلية على النحو التالي:
  1. بالنسبة لحالة وحدة فك التشفير الحالية #en_std([$s_i$])، يتم حساب قيمة #en_std([\"energy\"]) قياسية #en_std([$e_(i,u)$]) لكل خطوة زمنية #en_std([$u$]) من خرج المستمع #en_std([$h$]). تقيس هذه القيمة التوافق بين الحالة الحالية لوحدة فك التشفير وكل جزء من الصوت المشفر.
  2.  يتم توحيد قيم الطاقة باستخدام دالة #en_std([*softmax*]) لإنشاء توزيع احتمالي #en_std([$alpha_i$])، يُعرف باسم *متجه الانتباه*.
  3.  يتم حساب متجه السياق النهائي #en_std([$c_i$]) كمجموع موزون لمتجهات ميزات المستمع #en_std([$h_u$])، حيث تكون الأوزان هي احتمالات الانتباه #en_std([$alpha_(i,u)$]).
])

#ar_std([
  المعادلات التي تتحكم في هذه العملية هي:
])

#align(center)[
  #en_std([
    $e_(i,u) = <phi(s_i), psi(h_u)>$\ \
    $alpha_(i,u) = (exp(e_(i,u))) / (sum_u exp(e_(i,u)))$\ \
    $c_i = sum_u alpha_(i,u) h_u$
  ])
]

#ar_std([
  تصف المعادلات أعلاه *الانتباه القائم على المحتوى* الذي تم تحسينه باستخدام تقنيات أكثر حداثة موضحة في الأقسام اللاحقة.

  هنا، #en_std([$phi$]) و #en_std([$psi$]) هما بيرسيبترونات متعددة الطبقات (#en_std([MLPs])) صغيرة. تسمح هذه الآلية للمهجئ بأن يقرر ديناميكيًا أي إطارات من الصوت يجب ”الانتباه“ إليها عند إنتاج كل حرف، مما يخلق توافقًا مباشرًا وقابلًا للتعلم بين الصوت والنص.
])

#pagebreak()

#ar_std([
  الشكل الآتي يوضح بنية المهجئ وترابطه مع أداة الانتباهي:
])

#figure(
  image("../media/LAS_Speller-Architecture.png", width: 100%),
  kind: image,
  caption: flex_captions(
    ar_std([المفكك #en_clean_std([Speller]) القائم على الانتباه. في كل خطوة، يستخدم الحرف السابق #en_std([$y$])، وحالته الداخلية #en_std([$s$])، ومتجه السياق #en_std([$c$]) (الذي تم إنشاؤه بواسطة آلية الانتباه على #en_std([$h$])) لإنتاج الحرف التالي في التسلسل.]),
    ar_std([بنية المهجئ])
  )
)

#linebreak()

=== #ar([تحسين النتائج باستخدام تقنيات فك التشفير])

#ar_std([
  بالإضافة إلى بنية النموذج وتدريبه، يمكن أن تتأثر جودة الناتج النهائي بشكل كبير باستراتيجية فك التشفير المستخدمة أثناء الاستدلال. تهدف هذه التقنيات إلى إيجاد التسلسل الأكثر دقة من بين العدد الهائل من الاحتمالات التي يمكن أن يولدها النموذج.
])

#pagebreak()

==== #en_clean([Beam Search Decoding])

#ar_std([
  طريقة بسيطة لفك التشفير هي *البحث الجشع*، حيث يختار النموذج الحرف الأكثر احتمالاً في كل خطوة زمنية. ولكن هذا قد يؤدي إلى نتائج غير مثالية، حيث لا يمكن التراجع عن اختيار سيئ في البداية.

  طريقة أكثر فعالية هي #en_std([*beam search decoding*]). بدلاً من الاحتفاظ بفرضية واحدة فقط، يحافظ البحث الشعاعي على عدد محدد من التسلسلات الجزئية الأكثر احتمالاً، والمعروفة باسم ”الشعاع“. في كل خطوة، يتم توسيع كل تسلسل في الحزمة بكل حرف تالي ممكن، ويتم الاحتفاظ فقط بالتسلسلات الناتجة الأكثر احتمالًا في الحزمة الجديدة. وهذا يسمح للمفكك باستكشاف مساحة بحث أكبر بكثير والتعافي من القرارات السيئة محليًا، مما يؤدي غالبًا إلى نسخة نهائية أكثر دقة.
])

#linebreak()

==== #ar([دمج القاموس ونموذج لغة])

#ar_std([
  هناك طريقة أخرى لتحسين الدقة وهي دمج مجالات المعرفة الخارجية.

  - *قيود القاموس*: تتمثل إحدى الأفكار في استخدام قاموس لتقييد نطاق البحث على الكلمات الصحيحة فقط، وتصحيح الأخطاء الإملائية المحتملة التي يرتكبها النموذج. ولكن مؤلفي ورقة #en_std([LAS]) وجدوا أن هذا ليس ضروريًا. فقد تعلم نموذجهم تهجئة الكلمات الحقيقية بشكل صحيح في معظم الأحيان من تلقاء نفسه، وأظهرت التجارب أن تقييد البحث باستخدام قاموس لم يكن له أي تأثير على معدل أخطاء الكلمات (#en_std([WER])).
  - *إعادة تقييم نموذج اللغة*: هناك طريقة أكثر فعالية تتمثل في استخدام نموذج لغة خارجي لإعادة تقييم قائمة التسلسلات المرشحة التي أنتجها البحث الشعاعي. كما هو موضح بالتفصيل في القسم السابق، تعمل هذه التقنية على تعديل درجات كل فرضية بناءً على معقوليتها اللغوية، مما يحسن الدقة النهائية بشكل كبير.
])

#pagebreak()

== #en_clean([Scheduled Sampling])

#ar_std([
  عند تدريب الشبكات العصبونية التكرارية لمهام توليد التسلسلات، يوجد تباين كبير بين كيفية تدريب النموذج وكيفية استخدامه للاستدلال. يمكن أن تؤدي هذه الفجوة إلى تراكم الأخطاء عندما يولد النموذج سلاسل، حيث قد يواجه حالات أثناء الاستدلال لم يتعرض لها مطلقًا أثناء التدريب. #en_std([Scheduled Sampling]) هي استراتيجية تعليمية مصممة لسد هذه الفجوة عن طريق تغيير عملية التدريب تدريجيًا لجعل النموذج أكثر قوة في مواجهة أخطائه.
])

#linebreak()

=== #en_clean([Teacher Forcing])

#ar_std([
  الطريقة الموحدة والأكثر شيوعًا لتدريب شبكات #en_std([RNN]) المولدة للتسلسلات تُعرف باسم #en_std([*Teacher Forcing*]). في هذا النموذج، عند توقع الرمز التالي في التسلسل، #en_std([$y_t$])، يتم تزويد النموذج دائمًا بالرمز السابق الحقيقي، #en_std([$y_(t-1)$])، من بيانات التدريب. وبالتالي، فإن حالة النموذج في الوقت #en_std([$t$]) هي دالة للحالة السابقة والرمز السابق الصحيح:
])

#align(center)[
  #en_std([
    $h_t = f(h_(t-1), y_(t-1))$
  ])
]

#linebreak()

#ar_std([
  هذه الطريقة فعالة وتوجه النموذج بشكل فعال، حيث تتلقى الشبكة دائمًا المعلومات الصحيحة، مما يمنعها من الانحراف إلى أجزاء غير مرئية من فضاء الحالة. ومع ذلك، فإن هذا يخلق عدم توافق مع مرحلة الاستدلال. أثناء الاستدلال، لا تتوفر الرموز الحقيقية، ويجب أن يعتمد النموذج على الرموز التي تم إنشاؤها مسبقًا. إذا ارتكب النموذج خطأ في وقت مبكر، فقد يتفاقم الخطأ، حيث أن النموذج غير مدرب على التعافي من تنبؤاته غير الصحيحة.
])

#linebreak()

=== #ar([نموذج التشغيل الحر])

#ar_std([
  عكس #en_std([Teacher Forcing]) هو تدريب *حر تمامًا* أو *في وضع الاستدلال*. في هذا الإعداد، سيتم تدريب النموذج عن طريق تغذيته بتنبؤاته السابقة، #en_std([$hat(y)_(t-1)$]), كمدخلات للخطوة التالية. سيكون تحديث الحالة كما يلي:
])

#align(center)[
  #en_std([
    $h_t = f(h_(t-1), hat(y)_(t-1))$
  ])
]

#linebreak()

#ar_std([
  في حين أن هذا يعكس تمامًا الظروف في وقت الاستدلال، إلا أنه من الصعب للغاية تدريب نموذج بهذه الطريقة من البداية. ستنتج الشبكة غير المدربة في البداية رموزًا شبه عشوائية، مما يؤدي إلى مدخلات فوضوية ويجعل من الصعب جدًا على النموذج أن يتقارب للنتيجة المطلوبة. وقد ثبت أن هذه الطريقة، التي يشار إليها في البحث باسم #en_std([\"Always Sampling\"]) (الأخذ المستمر للعينات)، تؤدي إلى أداء سيئ للغاية.
])

#linebreak()

=== #ar([سد الفجوة من خلال #en_clean([Scheduled Sampling])])

#ar_std([
  طريقة #en_std([Scheduled Sampling]) توفر حلاً يربط بين طريقة #en_std([Teacher Forcing]) ونموذج التشغيل الحر. بدلاً من اختيار طريقة واحدة، يتم مزج الطريقتين أثناء التدريب عن طريق اتخاذ قرار عشوائي في كل خطوة زمنية بشأن تغذية النموذج برمز حقيقي أو برمز تم أخذه من التوزيع الناتج.

  يُشار إلى الاحتمال الذي يتحكم في هذا الاختيار بـ #en_std([$epsilon_i$]). من المهم ملاحظة كيفية تعريف هذا المصطلح في تنفيذ هذا المشروع.  وفقًا للورقة الأصلية، يصف الاحتمال #en_std([$epsilon_i$]) فرصة اختيار الرمز الحقيقي. ومع ذلك، لأغراض هذا المشروع، يتم تعريف الاحتمال #en_std([$epsilon_i$]) بشكل عكسي وتصف فرصة اختيار الرمز المتوقع من النموذج نفسه. وبالتالي، يتم اختيار رمز الحقيقي باحتمالية #en_std([$1 - epsilon_i$]).
])

#linebreak()

#ar_std([
  يأتي الجانب ”المجدول“ من استخدام منهج دراسي حيث يتم *زيادة* قيمة #en_std([$epsilon_i$]) تدريجياً أثناء التدريب وفقاً لجدول زمني متزايد.
  - *في بداية التدريب*، تكون قيمة #en_std([$epsilon_i$]) منخفضة (على سبيل المثال، قريبة من 0)، لذلك يعتمد النموذج بشكل كبير على #en_std([Teacher Forcing]) لتعلم الهيكل الأساسي للمهمة.
  - *مع تقدم التدريب*، تزداد قيمة #en_std([$epsilon_i$])، مما يجبر النموذج على الاعتماد بشكل متزايد على تنبؤاته الخاصة. تعلّم هذه العملية النموذج أن يصبح أكثر قوة وقدرة على تصحيح أخطائه، تمامًا كما هو مطلوب أثناء الاستدلال.
])

#pagebreak()

=== #ar([جدولة إيبسيلون])

#ar_std([
  لتنفيذ المنهج الدراسي، يجب زيادة الاحتمال #en_std([$epsilon_i$]) تدريجياً طوال عملية التدريب. وهذا يضمن انتقال النموذج بسلاسة من الاعتماد على البيانات الأساسية (#en_std([Teacher Forcing])) إلى الاعتماد على تنبؤاته الخاصة. تقترح الورقة الأصلية عدة جداول زمنية لهذا الانتقال. وبتكييفها مع تعريفنا حيث يزداد #en_std([$epsilon_i$]) بمرور الوقت، فإن هذه الجداول الزمنية هي:

  - *الزيادة الخطية:* تزداد الاحتمالية #en_std([$epsilon_i$]) بشكل خطي من قيمة البداية إلى القيمة القصوى على مدار عدد محدد من تكرارات التدريب. يمكن التعبير عن الصيغة على النحو التالي:\ #en_std([$epsilon_i = "min"(epsilon_max, k + c i)$]), حيث #en_std([$k$]) هي الاحتمالية الأولية، و#en_std([$c$]) هي معدل الزيادة، و#en_std([$epsilon_max$]) هي الحد الأعلى.

  - *الزيادة الأسية:* يتبع الاحتمال منحنى أسي، حيث يرتفع بسرعة في البداية ثم يستقر مع اقترابه من قيمته القصوى. الصيغة هي #en_std([$epsilon_i = 1 - k^i$])، حيث #en_std([$k < 1$]) هو ثابت يتحكم في معدل الزيادة.

  - *الزيادة السيغمويدية:* يستخدم هذا الجدول منحنى على شكل سيغمويد، والذي يوفر زيادة أولية بطيئة، تليها زيادة سريعة، وأخيرًا استقرار تدريجي مع اقترابه من قيمته القصوى. الصيغة هي #en_std([$epsilon_i = ("exp"(i/k)) / (k + "exp"(i/k))$]), حيث #en_std([$k >= 1$]) هو معامل يحدد انحدار المنحنى.
])

#linebreak()

#ar_std([
  يوضح الشكل التالي الاستراتيجيات الثلاث لزيادة احتمالية استخدام رموز التنبؤ على مدى 1000 حقبة.
  - تم رسم المخطط *الخطي* بقيمة #en_std([$k = 0$]) وقيمة #en_std([$c = 0.001$])، مما يظهر ارتفاعًا ثابتًا ومستمرًا.
  - تم رسم المخطط *الأسي* بقيمة #en_std([$k = 0.99$])، مما ينتج عنه زيادة سريعة في البداية قبل أن يستقر.
  - أخيرًا، يتم رسم مخطط #en_std([*Sigmoid*]) بقيمة #en_std([$k = 100$])، مما ينتج عنه شكل حرف #en_std([S]).
])

#figure(
  image("../media/Scheduled-Sampling_Increasing-Schedules.png",  width: 70%),
  kind: image,
  caption: flex_captions(
    ar([يوضح الرسم البياني كيف يزداد #en_std([$epsilon$]) على مدار 1000 حقبة تدريب وفقًا لكل جدول زمني.]),
    ar([مخططات #en_clean([Sampling Rate Schedules])])
  )
)

#linebreak()

=== #ar([التطبيق والتأثير على #en_clean([LAS])])

#ar_std([
  تتجلى الفوائد العملية لـ #en_std([\"Scheduled Sampling\"]) بوضوح في ورقة #en_std([\"Listen, Attend, and Spell\"]). طبق "لمؤلفون هذه التقنية - التي يطلقون عليها "حيلة العينات" - أثناء تدريب نموذج #en_std([LAS]) لسد الفجوة بين كيفية تعلم النموذج وكيفية أدائه للاستدلال.

  من خلال تدريب النموذج باحتمال #en_std([$10%$]) لاستخدام حرف تم أخذه من توزيع مخرجاته السابقة، حققوا انخفاضًا كبيرًا في معدل أخطاء الكلمات #en_std(([WER])). يقارن الجدول أدناه أداء نموذج #en_std([LAS]) القياسي مع الإصدار الذي تم تدريبه باستخدام العينات، باستخدام بيانات من الورقة البحثية الأصلية.
])

#figure(
  table(
    columns: 3,
    align: (center, center, center),
    en_std([*Model*]), en_std([*Clean WER*]), en_std([*Noisy WER*]),
    en_std([LAS (Baseline)]), en_std([$16.2%$]), en_std([$19.0%$]),
    en_std([LAS + Sampling]), en_std([$14.1%$]), en_std([$16.5%$]),
  ),
  kind: table,
  caption: flex_captions(
    ar_std([مقارنة #en_clean([WER]) لنموذج #en_clean([LAS]) مع وبدون أخذ #en_clean([Scheduled Sampling]).]),
    ar_std([مقارنة بين #en_clean([LAS]) الأساسي والمجدول])
  )
)

#linebreak()

#ar_std([
  يوضح هذا التطبيق المباشر أن #en_std([Scheduled Sampling]) ليست مجرد مفهوم نظري، بل هي تقنية قيّمة تؤدي إلى نماذج أكثر قوة ودقة لتوليد التسلسلات في الممارسة العملية.
])

#pagebreak()

== #ar([تحسين أداة المهجئ باستخدام آليات الانتباه المتقدمة])

#ar_std([
  يستخدم نموذج #en_std([LAS]) الأساسي آلية انتباه قائمة على المحتوى تسمح للمهجئ بالتركيز على الأجزاء ذات الأهمية من الإشارة الصوتية المشفرة. ورغم فعالية هذه الآلية الأساسية، إلا أنه يمكن تحسينها بشكل كبير باستخدام تقنيات أكثر تقدمًا، كل منها مصمم لحل تحدي معين في مهام التسلسل إلى التسلسل. يستكشف هذا القسم اثنين من هذه التحسينات التي تعتبر ضرورية لبناء نموذج متطور.
])

#linebreak()

=== #ar([تحسين قوة التمثيل: #en_clean([Multi-Head Attention])])

#ar_std([
  تم تقديم #en_std([*Multi-Head Attention*]) لأول مرة في ورقة بحثية بعنوان #en_std([\"Attention Is All You Need\"])، وهي تعزز قدرة النموذج على التقاط نطاق أوسع من الميزات من التسلسل المدخل. بدلاً من إجراء حساب انتباه واحد، تعمل هذه الآلية على تشغيل عمليات انتباه متعددة — أو ”رؤوس“ — بشكل متوازٍ.  يتعلم كل رأس أن يركز على جانب أو فضاء فرعي مختلف من تمثيل المدخلات.

  على سبيل المثال، في الكلام، قد يتعلم أحد الرؤوس التركيز على ميزات صوتية منخفضة المستوى، بينما قد يلتقط رأس آخر معلومات نغمية مثل النغمة أو الإيقاع. يتم بعد ذلك ربط مخرجات هذه الرؤوس المتوازية وتحويلها خطيًا إلى نموذج واحد. تسمح هذه البنية للنموذج بالاهتمام بشكل مشترك بالمعلومات من الفضاءات الفرعية المختلفة للتمثيل في مواقع مختلفة، مما ينتج عنه فهم أكثر ثراءً ودقة للمدخلات.
])

#linebreak()

=== #ar([تحسين التوافق: #en_clean([Location-Aware Attention])])

#ar_std([
  بالنسبة لمهام مثل التعرف على الكلام حيث تتوافق تسلسلات الإدخال والإخراج بشكل رتيب (أي في اتجاه أمامي ثابت)، قد تواجه آلية الانتباه القائمة على المحتوى وحده صعوبات في بعض الأحيان. فقد تعلق في الانتباه إلى نفس الجزء من الصوت، مما يتسبب في تكرار الكلمات، أو قد تقفز إلى الأمام، متخطية أجزاء من الإدخال بالكامل.
])

#pagebreak()

#ar_std([
  *الانتباه المدرك للموقع*، المقترح في #en_std([\"Attention-Based Models for Speech Recognition\"])، يعالج هذه المشكلة بشكل مباشر. فهو يعزز الآلية المعيارية من خلال جعلها مدركة لخياراتالتوافق السابقة. ويتحقق ذلك عن طريق تغذية أوزان الانتباه من الخطوة الزمنية السابقة كمدخلات إضافية عند حساب الانتباه للخطوة الزمنية الحالية. هذا التعديل البسيط ولكن القوي يجبر النموذج على إدراك ”موقعه“ في تسلسل المدخلات، مما يشجعه بشدة على المضي قدمًا بشكل متسق ويمنع أخطاء التوافق.
])

#linebreak()

#ar_std([
  هاتان الآليتان لا تتعارضان ويمكن دمجهما لإنشاء نظام انتباه عالي الفعالية. النموذج النهائي الذي تم تطويره لهذا المشروع يدمج كلاً من #en_std([*Multi-Head*]) و #en_std([*Location-Aware Attention*]). هذا النهج الهجين يستفيد من قدرات استخراج الميزات المحسنة لرؤوس متعددة مع ضمان محاذاة مستقرة ومتسقة من خلال الوعي بالموقع، مما ينتج عنه نموذج تدوين أكثر قوة ودقة بشكل عام.
])

#linebreak()

== #ar([#en_clean([SpecAugment]): تعزيز قوي للبيانات الخاصة بالكلام])

#ar_std([
  يتمثل أحد التحديات الكبيرة في تطوير نماذج التعرف على الكلام في ضمان أن تكون هذه النماذج قوية بما يكفي للتعامل مع تقلبات الصوت في العالم الحقيقي. يمكن أن تؤدي الاختلافات في لهجات المتحدثين وسرعة الكلام وضوضاء الخلفية إلى تدهور الأداء بشكل كبير. لمعالجة هذه المشكلة، قمنا بدمج #en_std([*SpecAugment*])، وهي تقنية بسيطة ولكنها قوية لتعزيز البيانات تعمل مباشرة على ميزات إدخال النموذج.

  تضيف #en_std([SpecAugment]) تشوهات إلى مخطط الطيف الصوتي #en_std([log-mel]) للصوت، مما يعلم النموذج أن يصبح ثابتًا أمام هذه التغيرات دون تغيير النص الأساسي. هذه الطريقة غير مكلفة من الناحية الحسابية ويمكن تطبيقها أثناء التدريب. وهي تتكون من ثلاث سياسات توسيع أساسية:
])

#linebreak()

=== #ar([تغيير الزمن])

#ar_std([
  تشويه الزمن هو تقنية تشوه الطيف الصوتي على طول محور الزمن. يتم اختيار نقطة عشوائية على طول محور الزمن، ويتم تشويه جميع الخطوات الزمنية إلى اليسار أو اليمين من تلك النقطة بمسافة عشوائية. وهذا يحاكي التغيرات في سرعة الكلام، مما يجبر النموذج على تعلم السمات التي لا تعتمد على نمط زمني ثابت.
])

#linebreak()

=== #ar([حجب التردد])

#ar_std([
  في حجب التردد، يتم اختيار مجموعة من قنوات التردد المتتالية وحجبها بقيمة صفر. وهذا مشابه لتغطية نطاق أفقي على مخطط الطيف الصوتي. من خلال القيام بذلك، نمنع النموذج من الاعتماد المفرط على معلومات تردد محددة، والتي قد لا تكون موجودة دائمًا بسبب الضوضاء أو الخصائص الصوتية الخاصة بالمتحدث. وهذا يشجع النموذج على تعلم ميزات أكثر توزيعًا وقوة.
])

#linebreak()

=== #ar([حجب الوقت])

#ar_std([
  على غرار حجب التردد، يتضمن حجب الوقت حجب مجموعة من الخطوات الزمنية المتتالية. وهذا يعادل تغطية نطاق عمودي على الطيف الصوتي. تجعل هذه التقنية النموذج أكثر مرونة في التعامل مع حالات الانسداد في المجال الزمني، مثل التوقفات القصيرة أو التلعثم أو الضوضاء القصيرة التي قد تحجب جزءًا من إشارة الكلام.
])

#linebreak()

#ar_std([
  من خلال تطبيق هذه التحويلات الثلاثة، يقوم #en_std([SpecAugment]) بإنشاء مجموعة أكثر تنوعًا من أمثلة التدريب من البيانات الموجودة. وهذا يساعد النموذج على التعميم بشكل أفضل على البيانات غير المرئية، مما يحسن بشكل كبير من متانته ويقلل من معدل أخطاء الكلمات (#en_std([WER]))، خاصة في الظروف الصاخبة أو غير المثالية.
])

#pagebreak()
