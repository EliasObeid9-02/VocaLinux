#import "../helpers.typ": flex_captions, ar, ar_std, en, en_std, en_clean, en_clean_std, ref

= #ar([الفصل الثاني: الأسس النظرية])

#ar_std([
  التعرف التلقائي على الكلام (#en_clean_std([ASR])) هو تقنية تمكّن الأجهزة من فهم الكلام البشري وتدوينه، وقد تطورت من مجال علمي متخصص إلى ميزة شائعة في الحياة العصرية. من المساعدين الافتراضيين على الهواتف الذكية إلى الأجهزة المنزلية التي يتم التحكم فيها بالصوت، غيرت أنظمة #en_clean_std([ASR]) بشكل جذري طريقة تفاعلنا مع التكنولوجيا. في جوهرها، تهدف أي نظام #en_clean_std([ASR]) إلى حل مشكلة معقدة تتعلق بالتعرف على الأنماط: تحويل موجة صوتية متواصلة ومتغيرة إلى تسلسل نصي منفصل ومنظم.

  يتعمق هذا الفصل في الأسس النظرية للتقنيات اللازمة لبناء نظام #en_clean_std([ASR]) حديث. سنبدأ بتتبع تطور بنى #en_clean_std([ASR]) لتوفير سياق للحالة الراهنة. بعد ذلك، سنستكشف المكونات والاستراتيجيات المحددة التي تشكل أساس هذا المشروع، من مجموعة البيانات وبنية النموذج إلى مبادئ التدريب والتقييم.
])

== #ar([تطور هياكل التعرف على الكلام])

#ar_std([
  شهد مجال التعرف على الكلام التلقائي (#en_clean_std([ASR])) تحولًا جوهريًا في العقدين الأخيرين. تمثل الرحلة من خطوط الإنتاج المعقدة متعددة المراحل إلى الشبكات العصبية الموحدة الشاملة تطورًا حاسمًا في السعي لتحقيق نسخ أكثر دقة وكفاءة للكلام.  
])  

=== #ar([النماذج الهجينة التقليدية])

#ar_std([
  لسنوات عديدة، كانت أحدث التقنيات في مجال التعرف على الكلام الآلي (#en_clean_std([ASR])) تهيمن عليها الأنظمة الهجينة التي تجمع بين عدة مكونات مدربة بشكل مستقل. وعادة ما تشمل هذه الأنظمة التقليدية ما يلي:
  - *نموذج صوتي*، غالبًا ما يكون نموذج مزيج غاوسي (#en_clean_std([GMM])) مقترنًا بنموذج ماركوف الخفي (#en_clean_std([HMM]))، وهو مسؤول عن تعيين الإطارات الصوتية القصيرة إلى وحدات صوتية.
  - *نموذج النطق* أو المعجم، الذي يوفر ربطًا بين الوحدات الصوتية والكلمات.
  - *نموذج اللغة*، وهو عادةً نموذج #en_clean_std([n-gram])، الذي يقدر احتمالية تسلسل الكلمات، مضيفًا السياق اللغوي إلى التنبؤات.

    على الرغم من قوة هذه الأنظمة في ذلك الوقت، إلا أنها كانت معقدة في البناء والصيانة. كان لا بد من تصميم كل مكون بعناية وتدريبه على بيانات مختلفة، كما أن التبعيات المبرمجة بينها جعلت خط العمل بأكمله هشًا وصعبًا للتحسين كوحدة واحدة.
])

=== #ar([ظهور البنى الهندسية الشاملة])

#ar_std([
  أحدث ظهور التعلم العميق ثورة في مجال التعرف على الكلام الآلي (#en_clean_std([ASR])) مع تطوير نماذج *شاملة*. تحل هذه الأنظمة محل سلسلة العمليات المعقدة للنماذج التقليدية بشبكة عصبية واحدة موحدة تتعلم تدوين الكلام مباشرة من المدخلات الصوتية. يوفر هذا النهج عدة مزايا رئيسية:
  - *البساطة*: لا حاجة إلى مكونات مصممة يدويًا مثل نماذج النطق.
  - *التحسين المشترك*: يمكن تدريب النظام بأكمله بشكل مشترك على مجموعات من الصوت والنص، مما يسمح للنموذج بتعلم تخطيط مباشر وتحسين جميع معلماته الداخلية من أجل هدف واحد هو التدوين الدقيق.
- *الأداء*: تفوقت النماذج الشاملة باستمرار على الأنظمة الهجينة التقليدية، ووضعت معايير جديدة للدقة.

  ظهرت بنيتان أساسيتان كرائدتين في مجال التغطية الشاملة:
  + *التصنيف الزمني الترابطي (#en_clean_std([CTC]))*: تستخدم النماذج القائمة على #en_clean_std([CTC]) دالة خسارة متخصصة تسمح للشبكة بمعالجة المحاذاة بين إطارات الصوت ذات الطول المتغير وتسلسلات النص الأقصر دون الحاجة إلى آلية انتباه منفصلة.
  + *نماذج التسلسل إلى التسلسل القائمة على الانتباه*: تستخدم هذه النماذج، مثل بنية *#en_clean_std([Listen, Attend, and Spell (LAS)])* التي تشكل جوهر هذا المشروع، آلية الانتباه لتعلم التوافق بين المدخلات الصوتية والنص الناتج في كل خطوة بشكل صريح.

  كان قرار التركيز على بنية #en_clean_std([LAS]) لهذا المشروع مدفوعًا بتصميمها الأنيق وفعاليتها المثبتة في إنتاج تدوينات دقيقة للغاية على مستوى الحرف، مما يجعلها أساسًا قويًا لبناء نظام التعرف على الكلام المخصص لمجال معين.
])

#pagebreak()

== #ar([مجموعات البيانات الأساسية: مجموعة #en_clean([LibriSpeech])])

#ar_std([
  تعد مجموعة *#en_clean_std([LibriSpeech])* أحد الأركان الأساسية في أبحاث التعرف التلقائي على الكلام (#en_clean_std([ASR])) الحديثة. وقد تم تقديمها لأول مرة بواسطة #en_clean_std([Vassil Panayotov]) وآخرون، وهي عبارة عن مجموعة بيانات واسعة النطاق تحتوي على ما يقارب #en_clean_std([$1000$]) ساعة من الكلام الإنجليزي المقروء، مستمدة من الكتب الصوتية الخاصة بمشروع #en_clean_std([LibriVox]). وقد أصبحت هذه المجموعة، بفضل حجمها الكبير وتسجيلاتها الصوتية النقية وتقسيماتها المحددة جيدًا، معيارًا قياسيًا لتطوير وتقييم أنظمة #en_clean_std([ASR]). #ref("7")
])

=== #ar([هيكل المجموعة وتنظيمها])

#ar_std([
  يتم تنظيم مجموعة #en_clean_std([LibriSpeech]) في بنية نظام ملفات هرمية منطقية وسهلة التحليل. يتم فرز البيانات حسب رقم تعريف المتحدث ورقم تعريف الفصل، مما يضمن تجميع جميع العبارات المنطوقة في فصل واحد معًا. هذه البنية مهمة للغاية للباحثين الذين يرغبون في إجراء تحليل أو تكييف خاص بالمتحدث.

  هيكل المجلدات القياسي هو كما يلي:
  #align(center, en_std(```
    data_root/
    ├── split_name/  // e.g., dev-clean, test-clean, train-clean-100
    │   ├── speaker_id/
    │   │   ├── chapter_id/
    │   │   │   ├── speaker_id-chapter_id-utterance_id.flac
    │   │   │   └── speaker_id-chapter_id.trans.txt
  ```)
  )

  يحتوي كل مجلد فصل على عدة ملفات صوتية بتنسيق `.flac`، تمثل عبارات فردية، وملف واحد بتنسيق `.trans.txt`. يحتوي ملف النص هذا على النص المقابل لجميع العبارات الصوتية في ذلك الفصل، مع ربط كل اسم ملف صوتي بنصه الأصلي.
])


=== #ar([تقسيمات البيانات المعيارية])

#ar_std([
  يتم تقسيم المجموعة رسميًا إلى عدة أقسام محددة مسبقًا، كل منها يخدم غرضًا محددًا في دورة تطوير النموذج. يتم تصنيف الأقسام الأساسية إلى ”نقية“ (أقل ضوضاء خلفية وصدى) و”أخرى“ (صوت أكثر صعوبة).

  #figure(
    table(
      columns: (2fr, 0.75fr, 1.25fr, 4fr),
      align: (center+horizon, center+horizon, center+horizon, center+horizon),
      [*اسم القسم*], [*الساعات*], [*المتحدثين*], [*الهدف*],
      [`train-clean-100`], [$100$], [$251$], [مقاطع صوتية نقية للتدريب البسيط.],
      [`train-clean-360`], [$360$], [$921$], [مقاطع صوتية نقية للتدريب الطويل.],
      [`train-other-500`], [$500$], [$1166$], [مقاطع صوتية صعبة للتدريب.],
      [`dev-clean`], [$5.4$], [$40$], [مقاطع صوتية نقية لمهام #en_clean_std([validation]) و #en_clean_std([development]).],
      [`dev-other`], [$5.1$], [$39$], [مقاطع صوتية صعبة لمهام #en_clean_std([validation]) و #en_clean_std([development]).],
      [`test-clean`], [$5.4$], [$40$], [مقاطع صوتية نقية لتجريب النموذج.],
      [`test-other`], [$5.1$], [$40$], [مقاطع صوتية صعبة لتجريب النموذج.],
    ),
    caption: flex_captions(
      ar_std([تقسيمات البيانات المتاحة في مجموعة #en_clean_std([LibriSpeech]).]),
      ar_std([تقسيم بيانات مجموعة #en_clean_std([LibriSpeech])])
    )
  )
  
  يتيح هذا التقسيم المعياري إجراء تجارب قابلة للتكرار ومقارنة عادلة بين نماذج ومنهجيات #en_clean_std([ASR]) المختلفة.
])

== #ar([#en_clean([SpecAugment]): تعزيز قوي للبيانات الخاصة بالكلام])

#ar_std([
  يتمثل أحد التحديات الكبيرة في تطوير نماذج التعرف على الكلام في ضمان أن تكون هذه النماذج قوية بما يكفي للتعامل مع تقلبات الصوت في العالم الحقيقي. يمكن أن تؤدي الاختلافات في لهجات المتحدثين وسرعة الكلام وضوضاء الخلفية إلى تدهور الأداء بشكل كبير. لمعالجة هذه المشكلة، قمنا بدمج #en_clean_std([*SpecAugment*])، وهي تقنية بسيطة ولكنها قوية لتعزيز البيانات تعمل مباشرة على ميزات إدخال النموذج.

  تضيف #en_clean_std([SpecAugment]) تشوهات إلى مخطط الطيف الصوتي #en_clean_std([log-mel]) للصوت، مما يعلم النموذج أن يصبح ثابتًا أمام هذه التغيرات دون تغيير النص الأساسي. هذه الطريقة غير مكلفة من الناحية الحسابية ويمكن تطبيقها أثناء التدريب. وهي تتكون من ثلاث سياسات توسيع أساسية:
])

=== #ar([تغيير الزمن])

#ar_std([
  تشويه الزمن هو تقنية تشوه الطيف الصوتي على طول محور الزمن. يتم اختيار نقطة عشوائية على طول محور الزمن، ويتم تشويه جميع الخطوات الزمنية إلى اليسار أو اليمين من تلك النقطة بمسافة عشوائية. وهذا يحاكي التغيرات في سرعة الكلام، مما يجبر النموذج على تعلم السمات التي لا تعتمد على نمط زمني ثابت.
])

=== #ar([حجب التردد])

#ar_std([
  في حجب التردد، يتم اختيار مجموعة من قنوات التردد المتتالية وحجبها بقيمة صفر. وهذا مشابه لتغطية نطاق أفقي على مخطط الطيف الصوتي. من خلال القيام بذلك، نمنع النموذج من الاعتماد المفرط على معلومات تردد محددة، والتي قد لا تكون موجودة دائمًا بسبب الضوضاء أو الخصائص الصوتية الخاصة بالمتحدث. وهذا يشجع النموذج على تعلم ميزات أكثر توزيعًا وقوة.
])

=== #ar([حجب الوقت])

#ar_std([
  على غرار حجب التردد، يتضمن حجب الوقت حجب مجموعة من الخطوات الزمنية المتتالية. وهذا يعادل تغطية نطاق عمودي على الطيف الصوتي. تجعل هذه التقنية النموذج أكثر مرونة في التعامل مع حالات الانسداد في المجال الزمني، مثل التوقفات القصيرة أو التلعثم أو الضوضاء القصيرة التي قد تحجب جزءًا من إشارة الكلام.

  #linebreak()

  من خلال تطبيق هذه التحويلات الثلاثة، يقوم #en_clean_std([SpecAugment]) بإنشاء مجموعة أكثر تنوعًا من أمثلة التدريب من البيانات الموجودة. وهذا يساعد النموذج على التعميم بشكل أفضل على البيانات غير المرئية، مما يحسن بشكل كبير من متانته ويقلل من معدل أخطاء الكلمات (#en_clean_std([WER]))، خاصة في الظروف الصاخبة أو غير المثالية.
])


== #ar([بنية الاستماع والانتباه والتهجئة (#en_clean([LAS]))])

#ar_std([
  نموذج الاستماع والانتباه والتهجئة (#en_clean_std([LAS])) هو بنية شبكة عصبية مصممة لتحويل الكلام المنطوق مباشرة إلى تسلسلات حروف. وهو نظام شامل يتعلم جميع مكونات أداة التعرف على الكلام بشكل مشترك، بعيدًا عن نماذج #en_clean_std([DNN-HMM]) التقليدية التي يتم تدريبها بشكل منفصل.

  جوهر #en_clean_std([LAS]) هو إطار عمل *سلسلة إلى سلسلة* يتكون من وحدتين أساسيتين: وحدة تشفير تسمى *المستمع* ووحدة فك تشفير تسمى *المهجئ*. يقوم المستمع بمعالجة الصوت المدخل، ويقوم المهجئ بإنشاء النص المقابل،حرفًا بحرف.

  يسعى النموذج إلى ربط تسلسل مدخلات طيف بنك المرشحات، #en_clean_std([$x = (x_1, ..., x_T)$]), بتسلسل مخرجات الأحرف، #en_clean_std([$y = (y_1, ..., y_S)$]). ويتم تحقيق ذلك من خلال نمذجة الاحتمال الشرطي باستخدام قاعدة التسلسل:

  #math.equation(
    [
      $P(y|x) = product_i P(y_i|x, y_(<i))$
    ],
    block: true
  )

  تقوم الدالتان الرئيسيتان، `Listen` و `AttendAndSpell`، بتحويل الإشارة المدخلة #en_clean_std([$x$]) إلى تمثيل عالي المستوى #en_clean_std([$h$]) ثم تنتجان التوزيع الاحتمالي النهائي على تسلسل الأحرف.

  #math.equation(
    [
        $h = "Listen"(x)$\
        $P(y|x) = "AttendAndSpell"(h, y)$
    ],
    block: true
  )

  الشكل التالي يوضح بنية نموذج #en_clean_std([LAS]). ويظهر الهيكل الهرمي للمستمع وربط مخرجات المستمع بمدخلات المهجئ.

  #figure(
    image("../media/LAS_Full-Architecture.png", width: 75%),
    kind: image,
    caption: flex_captions(
      [البنية الشاملة لنموذج الاستماع والمتابعة والتهجئة (#en_clean_std([LAS]))، والتي توضح التدفق من المستمع إلى المهجئ. #ref("9")],
      [بنية #en_clean_std([LAS]) الكاملة]
    )
  )

  #pagebreak()
])

=== #ar([المستمع (المشفّر)])

#ar_std([
  يعمل *المستمع* كجهاز تشفير للنموذج الصوتي. وتتمثل مهمته الأساسية في تحويل الخصائص الصوتية منخفضة المستوى من إشارة الكلام المدخلة #en_clean_std([$x$]) إلى تمثيل للخصائص عالية المستوى #en_clean_std([$h = (h_1, ..., h_U)$]).

  يتمثل أحد التحديات الرئيسية في التعرف على الكلام في طول إشارة الإدخال، التي يمكن أن تصل إلى آلاف الخطوات الزمنية. ستواجه شبكة #en_clean_std([RNN]) القياسية صعوبة في استخراج المعلومات ذات الصلة من تسلسل طويل كهذا. لمعالجة هذه المشكلة، يستخدم مستمع #en_clean_std([LAS]) بنية متخصصة تسمى شبكة #en_clean_std([*pyramidal Bidirectional Long Short-Term Memory* (pBLSTM)]).

  الغرض الرئيسي من #en_clean_std([pBLSTM]) هو *تقليل الدقة الزمنية* لتسلسل الميزات، مما يجعل السلسلة #en_clean_std([$h$]) أقصر من المدخلات #en_clean_std([$x$]) (أي #en_clean_std([$U < T$])). في كل طبقة متتالية من #en_clean_std([pBLSTM])، يتم تقليل الدقة الزمنية بمعامل #en_clean_std([$2$]).

  يتم تحقيق ذلك عن طريدمج المخرجات من الخطوات الزمنية المتتالية في طبقة واحدة قبل إدخالها كمدخلات إلى الطبقة التالية. يتم تعريف قاعدة التحديث في طبقة #en_clean_std([pBLSTM]) في المستوى #en_clean_std([$j$]) في الخطوة الزمنية #en_clean_std([$i$]) على النحو التالي:

  #math.equation(
    [
      $h_i^j = "pBLSTM"(h_(i-1)^j, [h_(2i)^(j-1), h_(2i+1)^(j-1)])$
    ],
    block: true
  )

  في تطبيق الورقة البحثية، يكدس المستمع ثلاث طبقات #en_clean_std([pBLSTM]) فوق طبقة #en_clean_std([BLSTM]) أولية، مما يقلل الدقة الزمنية النهائية بمعامل #en_clean_std([$2^3 = 8$]). هذا التخفيض الكبير في الطول يساعد آلية الانتباه على التركيز بسهولة أكبر، ويقلل أيضًا من التعقيد الحسابي العام للنموذج.

  #pagebreak()

  الشكل الآتي يوضح الطبقات الهرمية في المستمع وعملية تخفيف الدقة الزمنية.

  #figure(
    image("../media/LAS_Listener-Architecture.png", width: 50%),
    kind: image,
    caption: flex_captions(
      [بنية المشفر #en_clean_std([Listener]). يستخدم #en_clean_std([BLSTM]) هرمي لتقليل الدقة الزمنية لميزات الإدخال #en_clean_std([$x$]) تدريجياً، مما ينتج عنه تسلسل ميزات أقصر وعالي المستوى #en_clean_std([$h$]) لبرنامج #en_clean_std([Speller]).],
      [بنية المستمع]
    )
  )
])

=== #ar([المُهجّئ (المُفكّك)])

#ar_std([
  *المدقق الإملائي* هو مفكك رموز *قائم على الانتباه* يتلقى تمثيل السمات عالية المستوى #en_clean_std([$h$]) من المستمع ويقوم بإنشاء النص النهائي حرفًا حرفًا.

  في كل خطوة خرج #en_clean_std([$i$])، يحسب المدقق الإملائي توزيع الاحتمالات على جميع الأحرف الممكنة، بناءً على خرج المستمع #en_clean_std([$h$]) وجميع الأحرف التي تم إنشاؤها مسبقًا #en_clean_std([$y_(<i)$]). تدار هذه العملية بواسطة شبكة عصبية متكررة (#en_clean_std([RNN])) (على وجه التحديد، #en_clean_std([LSTM]) ذات طبقتين) تحافظ على حالة المفكك #en_clean_std([$s_i$]) وتستخدم متجه السياق #en_clean_std([$c_i$]) المقدم من آلية الانتباه. 

  العلاقة بين الحالة #en_clean_std([$s_i$]) للمفكك، ومتجه السياق #en_clean_std([$c_i$])، واحتمال الحرف النهائي هي كما يلي:

  #math.equation(
    [
        $s_i = "RNN"(s_(i-1), y_(i-1), c_(i-1))$\
        $P(y_i|x, y_(<i)) = "CharacterDistribution"(s_i, c_i)$
    ],
    block: true
  )

  تقوم آلية الانتباه بتوليد متجه سياق #en_clean_std([$c_i$]) في كل خطوة. يزود هذا المتجه أداة التهجئة بمعلومات مركزة من الأجزاء ذات الصلة من الصوت المدخل اللازمة لتوليد الحرف التالي.

  تعمل الآلية على النحو التالي:
  + بالنسبة لحالة وحدة فك التشفير الحالية #en_clean_std([$s_i$])، يتم حساب قيمة #en_clean_std([\"energy\"]) قياسية #en_clean_std([$e_(i,u)$]) لكل خطوة زمنية #en_clean_std([$u$]) من خرج المستمع #en_clean_std([$h$]). تقيس هذه القيمة التوافق بين الحالة الحالية لوحدة فك التشفير وكل جزء من الصوت المشفر.
  +  يتم توحيد قيم الطاقة باستخدام دالة #en_clean_std([*softmax*]) لإنشاء توزيع احتمالي #en_clean_std([$alpha_i$])، يُعرف باسم *متجه الانتباه*.
  +  يتم حساب متجه السياق النهائي #en_clean_std([$c_i$]) كمجموع موزون لمتجهات ميزات المستمع #en_clean_std([$h_u$])، حيث تكون الأوزان هي احتمالات الانتباه #en_clean_std([$alpha_(i,u)$]).

  المعادلات التي تتحكم في هذه العملية هي:

  #math.equation(
    [
      $e_(i,u) = <phi(s_i), psi(h_u)>$\
      $alpha_(i,u) = (exp(e_(i,u))) / (sum_u exp(e_(i,u)))$\
      $c_i = sum_u alpha_(i,u) h_u$
    ],
    block: true
  )

  تصف المعادلات أعلاه *الانتباه القائم على المحتوى* الذي تم تحسينه باستخدام تقنيات أكثر حداثة موضحة في الأقسام اللاحقة.

  هنا، #en_clean_std([$phi$]) و #en_clean_std([$psi$]) هما بيرسيبترونات متعددة الطبقات (#en_clean_std([MLPs])) صغيرة. تسمح هذه الآلية للمهجئ بأن يقرر ديناميكيًا أي إطارات من الصوت يجب ”الانتباه“ إليها عند إنتاج كل حرف، مما يخلق توافقًا مباشرًا وقابلًا للتعلم بين الصوت والنص.

  #pagebreak()

  الشكل الآتي يوضح بنية المهجئ وترابطه مع أداة الانتباهي:

  #figure(
    image("../media/LAS_Speller-Architecture.png", width: 50%),
    kind: image,
    caption: flex_captions(
      [المفكك #en_clean_std([Speller]) القائم على الانتباه. في كل خطوة، يستخدم الحرف السابق #en_clean_std([$y$])، وحالته الداخلية #en_clean_std([$s$])، ومتجه السياق #en_clean_std([$c$]) (الذي تم إنشاؤه بواسطة آلية الانتباه على #en_clean_std([$h$])) لإنتاج الحرف التالي في التسلسل.],
      [بنية المهجئ]
    )
  )
])

== #ar([تحسين أداة المهجئ باستخدام آليات الانتباه المتقدمة])

#ar_std([
  يستخدم نموذج #en_clean_std([LAS]) الأساسي آلية انتباه قائمة على المحتوى تسمح للمهجئ بالتركيز على الأجزاء ذات الأهمية من الإشارة الصوتية المشفرة. ورغم فعالية هذه الآلية الأساسية، إلا أنه يمكن تحسينها بشكل كبير باستخدام تقنيات أكثر تقدمًا، كل منها مصمم لحل تحدي معين في مهام التسلسل إلى التسلسل. يستكشف هذا القسم اثنين من هذه التحسينات التي تعتبر ضرورية لبناء نموذج متطور.
])

=== #ar([تحسين قوة التمثيل: #en_clean([Multi-Head Attention])])

#ar_std([
  تم تقديم #en_clean_std([*Multi-Head Attention*]) لأول مرة في ورقة بحثية بعنوان #en_clean_std([\"Attention Is All You Need\"])، وهي تعزز قدرة النموذج على التقاط نطاق أوسع من الميزات من التسلسل المدخل. بدلاً من إجراء حساب انتباه واحد، تعمل هذه الآلية على تشغيل عمليات انتباه متعددة — أو ”رؤوس“ — بشكل متوازٍ.  يتعلم كل رأس أن يركز على جانب أو فضاء فرعي مختلف من تمثيل المدخلات.

  على سبيل المثال، في الكلام، قد يتعلم أحد الرؤوس التركيز على ميزات صوتية منخفضة المستوى، بينما قد يلتقط رأس آخر معلومات نغمية مثل النغمة أو الإيقاع. يتم بعد ذلك ربط مخرجات هذه الرؤوس المتوازية وتحويلها خطيًا إلى نموذج واحد. تسمح هذه البنية للنموذج بالاهتمام بشكل مشترك بالمعلومات من الفضاءات الفرعية المختلفة للتمثيل في مواقع مختلفة، مما ينتج عنه فهم أكثر ثراءً ودقة للمدخلات.
])

=== #ar([تحسين التوافق: #en_clean([Location-Aware Attention])])

#ar_std([
  بالنسبة لمهام مثل التعرف على الكلام حيث تتوافق تسلسلات الإدخال والإخراج بشكل رتيب (أي في اتجاه أمامي ثابت)، قد تواجه آلية الانتباه القائمة على المحتوى وحده صعوبات في بعض الأحيان. فقد تعلق في الانتباه إلى نفس الجزء من الصوت، مما يتسبب في تكرار الكلمات، أو قد تقفز إلى الأمام، متخطية أجزاء من الإدخال بالكامل.

  *الانتباه المدرك للموقع*، المقترح في #en_clean_std([\"Attention-Based Models for Speech Recognition\"])، يعالج هذه المشكلة بشكل مباشر. فهو يعزز الآلية المعيارية من خلال جعلها مدركة لخياراتالتوافق السابقة. ويتحقق ذلك عن طريق تغذية أوزان الانتباه من الخطوة الزمنية السابقة كمدخلات إضافية عند حساب الانتباه للخطوة الزمنية الحالية. هذا التعديل البسيط ولكن القوي يجبر النموذج على إدراك ”موقعه“ في تسلسل المدخلات، مما يشجعه بشدة على المضي قدمًا بشكل متسق ويمنع أخطاء التوافق.

  هاتان الآليتان لا تتعارضان ويمكن دمجهما لإنشاء نظام انتباه عالي الفعالية. النموذج النهائي الذي تم تطويره لهذا المشروع يدمج كلاً من #en_clean_std([*Multi-Head*]) و #en_clean_std([*Location-Aware Attention*]). هذا النهج الهجين يستفيد من قدرات استخراج الميزات المحسنة لرؤوس متعددة مع ضمان محاذاة مستقرة ومتسقة من خلال الوعي بالموقع، مما ينتج عنه نموذج تدوين أكثر قوة ودقة بشكل عام.
])

== #ar([مبادئ تدريب وتقييم النموذج])

#ar_std([
  يتضمن تدريب الشبكة العصبية تحسين معلماتها بشكل متكرر لتقليل الخطأ المعرف إلى أدنى حد، مع قياس أدائها في الوقت نفسه على البيانات غير المرئية. تعتمد هذه العملية على ثلاثة مكونات رئيسية: دالة الخسارة لتقدير الخطأ، والمحسّن لتوجيه عملية التعلم، ومقاييس التقييم لقياس الأداء.  
])

=== #ar([دالة الخسارة: #en_clean([Sparse Categorical Cross-Entropy])])

#ar_std([
  تحسب *دالة الخسارة* قيمة تمثل الفرق بين توقع النموذج والمعلومة الحقيقية. الهدف من التدريب هو تقليل هذه القيمة إلى الحد الأدنى.

  بالنسبة لمهام التصنيف متعددة الفئات مثل توقع الأحرف، فإن دالة الخسارة القياسية هي #en_clean_std([*Categorical Cross-Entropy*]). وهي فعالة عندما ينتج النموذج توزيعًا احتماليًا (عبر دالة #en_clean_std([Softmax])) عبر جميع الفئات الممكنة.

  يستخدم هذا المشروع متغيرًا شائعًا يسمى #en_clean_std([*Sparse Categorical Cross-Entropy*]). هذه النسخة متطابقة وظيفيًا ولكنها مصممة لسيناريوهات يتم فيها توفير التسميات الحقيقية كأعداد صحيحة (على سبيل المثال، `[23، 4، 15]`) بدلاً من متجهات مشفرة بشكل #en_clean_std([one-hot]) (على سبيل المثال، `[[0..1..0]، [0..1..0]]`). وهذا أكثر كفاءة من حيث الذاكرة ومناسب تمامًا لمهام التسلسل إلى التسلسل حيث تكون الأهداف عبارة عن أرقام تعريف أحرف صحيحة.
])

=== #ar([المُحسِّن: #en_clean([Adam])])

#ar_std([
  *المُحسِّن* هو خوارزمية تُعدِّل المعلمات الداخلية للنموذج (الأوزان) استجابةً لنتائج دالة الخسارة. ويتمثل هدفه في العثور على مجموعة من الأوزان التي تؤدي إلى أقل خسارة ممكنة.

  #pagebreak()

  مُحسِّن #en_clean_std([*Adam (Adaptive Moment Estimation)*]) هو خوارزمية فعالة للغاية ومستخدمة على نطاق واسع لتدريب الشبكات العصبية العميقة. وهو يجمع بين مزايا مُحسِّنين آخرين شائعين، هما #en_clean_std([AdaGrad]) و #en_clean_std([RMSProp])، من خلال الحفاظ على متوسطين متحركين منفصلين للانحدارات:
  + اللحظة الأولى (المتوسط)، التي تعمل مثل الكمون.
  + اللحظة الثانية (التباين غير المركزي)، والتي توفر معدل تعلم تكيفي لكل متغير.

  تسمح هذه التركيبة لـ #en_clean_std([Adam]) بالتقارب بسرعة والأداء القوي عبر مجموعة واسعة من المشاكل، مما يجعله خيارًا قياسيًا للنماذج المعقدة.
])

=== #ar([مقياس تقييم: معدل خطأ الحرف (#en_clean([CER]))])

#ar_std([
  يقيس معدل خطأ الحرف (#en_clean_std([CER])) الأخطاء على مستوى الحرف. ويتم حسابه من خلال مقارنة تسلسل الحروف المتوقع بالنص الأصلي وتجميع عدد الاستبدالات #en_clean_std([$S$]) والإدخالات #en_clean_std([$I$]) وعمليات الحذف #en_clean_std([$D$]) المطلوبة لتحويل أحدهما إلى الآخر. ثم يتم تطبيع هذا المجموع حسب العدد الإجمالي للحروف في النص الأصلي #en_clean_std([$N$]).

  #math.equation(
    [
      $"CER" = (S + D + I) / N$
    ],
    block: true
  )
])

=== #ar([مقياس تقييم: معدل خطأ الكلمات (#en_clean([WER]))])

#ar_std([
  معدل الخطأ في الكلمات (#en_clean_std([WER])) هو مقياس التقييم المعياري في مجال أنظمة التعرف على الكلام الآلي (#en_clean_std([ASR])). وهو يعمل بشكل مماثل لمقياس #en_clean_std([CER]) ولكن على مستوى الكلمات، حيث يحسب الحد الأدنى لعدد عمليات استبدال الكلمات وحذفها وإضافتها المطلوبة لمطابقة الجملة الأصلية، ويتم تطبيعه حسب العدد الإجمالي للكلمات في الجملة الأصلية.

  #math.equation(
    [
      $"WER" = (S + D + I) / N$
    ],
    block: true
  )

  انخفاض معدل الأخطاء في الكلمات (#en_clean_std([WER])) يشير إلى نظام تدوين أكثر دقة.
])

== #ar([التدريب المتقدم واستراتيجيات الاستدلال])

#ar_std([
  بالإضافة إلى بنية النموذج الأساسية ودورة التدريب الأساسية، يمكن استخدام العديد من الاستراتيجيات المتقدمة لتحسين الأداء. يغطي هذا القسم التقنيات التي تعالج تحديات محددة في توليد التسلسل، من تحسين متانة النموذج أثناء التدريب إلى تحسين جودة الناتج النهائي أثناء الاستدلال.
])

=== #ar([استراتيجية التدريب: #en_clean([Scheduled Sampling])])

#ar_std([
  غالبًا ما يوجد تباين كبير بين كيفية تدريب الشبكة العصبية المتكررة (#en_clean_std([RNN])) وكيفية استخدامها للاستدلال. استراتيجية #en_clean_std([Scheduled Sampling]) هي استراتيجية تعليمية مصممة لسد هذه الفجوة، مما يجعل النموذج أكثر قوة في مواجهة أخطائه من خلال تغيير عملية التدريب تدريجيًا.
])

==== #ar([الاختلاف بين التدريب والاستدلال])

#ar_std([
  يكمن جوهر المشكلة في مصدر المدخلات إلى وحدة فك التشفير في كل خطوة زمنية.
  - *إجبار المعلم*: توفر طريقة التدريب المعيارية الرمز السابق الحقيقي، #en_clean_std([$y_(t-1)$]), كمدخلات عند توقع الرمز التالي، #en_clean_std([$y_t$]) #ref("8"). هذه الطريقة فعالة ومستقرة، ولكنها تخلق عدم تطابق مع الاستدلال، حيث لا يتوفر الرمز الحقيقي. لا يتم تدريب النموذج أبدًا على التعافي من أخطائه.
  - *التشغيل الحر*: النهج المعاكس هو إدخال التنبؤ السابق للنموذج نفسه، #en_clean_std([$hat(y)_(t-1)$]), كمدخلات أثناء التدريب. على الرغم من أن هذا يعكس ظروف الاستدلال بشكل مثالي، إلا أنه من الصعب للغاية التدريب من الصفر، حيث أن التنبؤات العشوائية في البداية يمكن أن تؤدي إلى مدخلات فوضوية وتمنع التقارب. وقد ثبت أن هذه الطريقة تؤدي إلى أداء ضعيف.
])

==== #ar([سد الفجوة])

#ar_std([
  يوفر #en_clean_std([Scheduled Sampling]) حلاً وسطًا من خلال المزج بين هذين النهجين. في كل خطوة زمنية، يقرر بشكل عشوائي ما إذا كان سيغذي النموذج برمز حقيقي أو برمز تم توقعه.
  
  يُشار إلى الاحتمال الذي يحكم هذا الاختيار بـ #en_clean_std([$epsilon$]). من المهم ملاحظة كيفية تعريف هذا المصطلح في تنفيذ هذا المشروع. في حين أن الورقة الأصلية التي كتبها #en_clean_std([Bengio]) وآخرون تعرف #en_clean_std([$epsilon$]) على أنه احتمال اختيار رمز *الحقيقة الأساسية*، فإن هذا المشروع يعرّف #en_clean_std([$epsilon$]) على أنه احتمال اختيار *الرمز المتوقع من النموذج نفسه*. وبالتالي، يتم اختيار الحقيقة الأساسية باحتمال #en_clean_std([$1 - epsilon$]). 

  يأتي الجانب ”المجدول“ من استخدام منهج دراسي يتم فيه زيادة قيمة #en_clean_std([$epsilon$]) تدريجياً أثناء التدريب. في البداية، تكون قيمة #en_clean_std([$epsilon$]) منخفضة، ويعتمد النموذج على #en_clean_std([Teacher Forcing]). مع تقدم التدريب، تزداد قيمة #en_clean_std([$epsilon$])، مما يجبر النموذج على الاعتماد بشكل أكبر على تنبؤاته الخاصة، وبالتالي تعليمه أن يكون أكثر قوة وقدرة على تصحيح أخطائه. تقترح الورقة الأصلية عدة جداول زمنية لهذا الانتقال، مثل الزيادات الخطية والأسية والسيغمويدية.
])

==== #ar([التأثير على #en_clean([LAS])])

#ar_std([
  أظهر مؤلفو ورقة #en_clean_std([\"Listen, Attend, and Spell\"]) الفوائد العملية لهذه التقنية، التي أطلقوا عليها اسم ”حيلة العينات“. من خلال تدريب نموذجهم باحتمالية #en_clean_std([$10%$]) لاستخدام حرف تم توقعه، حققوا انخفاضًا كبيرًا في معدل أخطاء الكلمات (#en_clean_std([WER])).

  يقارن الجدول أدناه، باستخدام بيانات من الورقة الأصلية، أداء نموذج #en_clean_std([LAS]) مع وبدون أخذ العينات.

  #en_std([
    #figure(
      table(
        columns: (3fr, 2fr, 2fr),
        align: (center, center, center),
        [*Model*], [*Clean WER*], [*Noisy WER*],
        [LAS (Baseline)], [$16.2%$], [$19.0%$],
        [LAf + Sampling], [$14.1%$], [$16.5%$],
      ),
      kind: table,
      caption: flex_captions(
        ar_std([مقارنة #en_clean_std([WER]) لنموذج #en_clean_std([LAS]) مع وبدون #en_clean_std([Scheduled Sampling])، كما ورد في الورقة الأصلية.]),
        ar_std([مقارنة #en_clean_std([WER]) قبل وبعد تطبيق #en_clean_std([Scheduled Sampling])])
      )
    )
  ])

  تظهر هذه النتيجة أن #en_clean_std([Scheduled Sampling]) هي تقنية قيّمة تؤدي إلى نماذج توليد تسلسل أكثر قوة ودقة في الواقع العملي.
])

=== #ar([استراتيجية التدريب: معدلات التعلم الدورية (#en_clean([CLR]))])

#ar_std([
  يعتبر معدل التعلم على نطاق واسع أكثر المعلمات أهمية التي يجب تعديلها. تعتمد الطرق التقليدية غالبًا على معدل تعلم ثابت يتم تخفيضه يدويًا على فترات زمنية محددة (تناقص متدرج). يتطلب هذا النهج إجراء تجارب مكثفة للعثور على الجدول الزمني الأمثل. توفر معدلات التعلم الدورية (#en_clean_std([CLR])) بديلاً أكثر منهجية وأكثر فعالية في كثير من الأحيان.
])

==== #ar([طريقة #en_clean([CLR])])

#ar_std([
  بدلاً من تقليل معدل التعلم، يقوم #en_clean_std([CLR]) بتغييره بشكل دوري بين حدين محددين مسبقًا: الحد الأدنى (`base_lr`) والحد الأقصى (`max_lr`). الفكرة وراء هذه الطريقة هي أن زيادة معدل التعلم بشكل دوري يمكن أن يكون لها تأثير سلبي قصير المدى على الأداء، ولكنها مفيدة على المدى الطويل، حيث تساعد عملية التحسين على تجنب نقاط التوازن أو الحدود الدنيا المحلية الحادة وإيجاد حل نهائي أفضل. #ref("10")

  يمكن استخدام عدة أشكال عملية للدورة، حيث أظهرت الورقة الأصلية أن النافذة المثلثية البسيطة (زيادة خطية تليها انخفاض خطي) فعالة للغاية. هناك نوع آخر شائع وقوي وهو #en_clean_std([*Cosine Annealing*])، حيث يتبع معدل التعلم منحنى دالة الجيب بين الحدود.
])

==== #ar([اختبار مجال #en_clean([LR])])

#ar_std([
  إحدى المساهمات الرئيسية لورقة #en_clean_std([CLR]) هي طريقة بسيطة وفعالة لإيجاد حدود معدل التعلم الأمثل: *اختبار نطاق #en_clean_std([LR])*. يتضمن ذلك تشغيل النموذج لعدة فترات مع زيادة معدل التعلم خطيًا من قيمة منخفضة جدًا إلى قيمة عالية. من خلال رسم الدقة (أو الخسارة) مقابل معدل التعلم، يمكن تحديد ما يلي بصريًا:
  - معدل التعلم الذي تبدأ عنده الدقة في الزيادة (خيار جيد لـ `base_lr`).
  - معدل التعلم الذي تبدأ عنده الدقة في التباطؤ أو التقلب أو الانخفاض (خيار جيد لـ `max_lr`).

  هذا الاختبار البسيط يلغي عمليًا الحاجة إلى إجراء تجارب متكررة مكثفة لإيجاد قيم جيدة لمعدل التعلم.
])

=== #ar([استراتيجية الاستدلال: فك التشفير بالبحث الشعاعي])

#ar_std([
  أثناء الاستدلال، الهدف هو العثور على التسلسل الأكثر دقة من بين العدد الهائل من الاحتمالات التي يمكن أن يولدها النموذج. البحث الجشع البسيط، الذي يختار الحرف الأكثر احتمالاً في كل خطوة، يمكن أن يؤدي إلى نتائج دون المستوى الأمثل لأن الخطأ المبكر لا يمكن إصلاحه.

  طريقة أكثر فعالية هي *فك التشفير بالبحث الشعاعي*. بدلاً من تتبع مسار واحد فقط، يحافظ البحث الشعاعي على عدد محدد من التسلسلات الجزئية الأكثر احتمالاً — ”الشعاع“. في كل خطوة، يتم توسيع كل تسلسل في الحزمة بجميع الأحرف التالية الممكنة، ويتم الاحتفاظ فقط بالتسلسلات الناتجة الأكثر احتمالاً للخطوة التالية. وهذا يسمح للمفكك باستكشاف مساحة بحث أكبر بكثير والتعافي من القرارات السيئة محليًا، مما يؤدي غالبًا إلى نسخة نهائية أكثر دقة.
])

=== #ar([استراتيجية الاستدلال: دمج نموذج اللغة])

#ar_std([
  هناك طريقة أخرى لتحسين الدقة وهي دمج مصادر المعرفة الخارجية أثناء فك التشفير. وهناك طريقتان شائعتان هما التصحيح باستخدام القاموس وإعادة التقييم باستخدام نموذج اللغة.
  - *تصحيح باستخدام القاموس*: يمكن استخدام القاموس لتقييد نطاق البحث على الكلمات الصحيحة فقط، وتصحيح الأخطاء الإملائية. ومع ذلك، وجد مؤلفو ورقة #en_clean_std([LAS]) أن هذا لا يؤثر على معدل الأخطاء الإملائية (#en_clean_std([WER]))، حيث تعلم نموذجهم التهجئة الصحيحة من تلقاء نفسه.
  - *إعادة تقييم باستخدام نموذج اللغة*: هناك تقنية أكثر فعالية تتمثل في استخدام نموذج لغة خارجي تم تدريبه بشكل مستقل لإعادة تقييم قائمة التسلسلات المرشحة التي تم إنتاجها بواسطة البحث الشعاعي. يقوم نموذج اللغة بتقييم المعقولية اللغوية لكل فرضية وتعديل درجتها وفقًا لذلك. وقد ثبت أن هذه الطريقة تحسن الدقة النهائية بشكل كبير.
])

#pagebreak()
