#import "../helpers.typ": flex_captions, ar, ar_std, en, en_std, en_clean, en_clean_std

= #ar([الفصل الثالث: التطبيق العملي])

#ar_std([
  يشرح هذا الفصل تفاصيل تصميم النظام وتنفيذه، ويغطي البنية الهندسية وتدريب النموذج وتكامل المكونات الأساسية. المكون الأول الذي سنناقشه هو المترجم القائم على القواعد، الذي يترجم ناتج نموذج التعرف على الكلام إلى أوامر قابلة للتنفيذ.
])

#linebreak()

== #ar([المترجم القائم على القواعد])

#ar_std([
  بينما يقوم نموذج #en_std([LAS]) بتحويل الكلام إلى سلسلة من الكلمات، فإن هذا النص لا يمثل بعد أمرًا فعالاً. المترجم القائم على القواعد هو المكون الأساسي الثاني للنظام، وهو مصمم لتحليل هذا النص باللغة الطبيعية وترجمته إلى أمر لينكس قابل للتنفيذ وصحيح من الناحية النحوية.

  يعمل المترجم على مجموعة من القواعد الحتمية، حيث يقسم النص المترجم إلى أجزاء منطقية متميزة بناءً على الكلمات الرئيسية. تقسم عملية التحليل المدخلات إلى أربعة أجزاء كحد أقصى:
  - *قسم الأوامر*: هذا هو المكون الأساسي المسؤول عن تحديد الأمر الأساسي. يتعرف على مجموعة محددة مسبقًا من الكلمات الرئيسية (مثل #en_std([\"list\"]) و #en_std([\"change directory\"]) و #en_std([\"copy\"])) ويقوم بتعيينها إلى أوامر لينكس المقابلة لها (مثل #en_std([`ls`]) و #en_std([`cd`]) و #en_std([`cp`])).
  - *قسم الخيارات*: يتعامل هذا القسم مع خيارات الأوامر المختصرة. يعالج النص الذي يتبع كلمة رئيسية ’خيارات‘ لتحديد العلامات أحادية الحرف، والتي يتم بعد ذلك إضافتها بشرطة (على سبيل المثال، الكلمة #en_std([\"a\"]) تصبح #en_std([`-a`])).
  - *قسم الخيارات الطويلة*: على غرار قسم الخيارات، يتعامل هذا القسم مع الخيارات الطويلة. يتعرف على الكلمات الرئيسية مثل #en_std([\"help\"]) أو #en_std([\"all\"]) ويضيف أمامها شرطتين لتشكيل خيارات طويلة صالحة (على سبيل المثال، #en_std([`--help`])، #en_std([`--all`])).
  - *قسم الحجج*: هذا القسم الأخير مسؤول عن إنشاء معاملات الأوامر، وأكثرها شيوعًا مسارات الملفات أو المجلدات. يتعرف على كلمات رئيسية صوتية محددة ويستبدلها بالأحرف الصحيحة. على سبيل المثال، يتم تحويل الكلمة المنطوقة #en_std([\"slash\"]) إلى الحرف #en_std([`/`])، مما يسمح للمستخدمين بإنشاء مسارات معقدة شفهيًا.
])

#pagebreak()

=== Escaping Keywords

To handle cases where a keyword (like "list" or "slash") is intended as a literal argument rather than a command or special character, the system includes an escape mechanism. The keyword "backslash" preceding any other recognized keyword causes the interpreter to treat the subsequent keyword as plain text, ignoring its special function. For example, the phrase "copy backslash list to home" would result in the command `cp list /home`, treating "list" as a literal filename.

#linebreak()

=== #ar([مفردات الأوامر المعترف بها])

#ar_std([
  تم تصميم المترجم الفوري للتعرف على مجموعة محددة من الكلمات الرئيسية المنطوقة وتعيينها إلى مفردات محددة مسبقًا من أوامر لينكس. يوضح الجدول التالي مجموعة فرعية تمثيلية من الأوامر المدعومة والكلمات الرئيسية المنطوقة المقابلة لها ووصفًا موجزًا لوظيفتها.
])

#linebreak()

#figure(
  table(
    columns: (1fr, 1fr, 2fr),
    align: (center+horizon, center+horizon, center+horizon),
    ar_std([*الكلمة المنطوقة*]), ar_std([*الأمر الذي تم إنشاؤه*]), ar_std([*الشرح*]),
    en_std([\"list\"]), en_std([`ls`]), ar_std([يعرض محتويات المجلد.]),
    en_std([\"change directory\"]), en_std([`cd`]), ar_std([يغير المجلد الحالي.]),
    en_std([\"copy\"]), en_std([`cp`]), ar_std([ينسخ الملفات أو المجلدات.]),
    en_std([\"move\"]), en_std([`mv`]), ar_std([ينقل أو يعيد تسمية الملفات أو المجلدات.]),
    en_std([\"remove\"]), en_std([`rm`]), ar_std([يزيل الملفات أو المجلدات.]),
    en_std([\"make directory\"]), en_std([`mkdir`]), ar_std([ينشئ مجلدًا جديدًا.]),
    en_std([\"echo\"]), en_std([`echo`]), ar_std([يعرض سطرًا من النص.]),
    en_std([\"touch\"]), en_std([`touch`]), ar_std([ينشئ ملفًا فارغًا.]),
  ),
  caption: flex_captions(
    ar_std([قائمة الأوامر التي يتعرف عليها المترجم.]),
    ar_std([الأوامر المعترف بها])
  )
)
 
== #ar([البيانات])

#ar_std([
  أساس أي نموذج للتعلم العميق هو البيانات التي يتم تدريبه عليها. في هذا المشروع، نستخدم مجموعة #en_std([*LibriSpeech*])، وهي مجموعة بيانات واسعة النطاق مستمدة من الكتب الصوتية الإنجليزية المقروءة من مشروع #en_std([LibriVox]). تسجيلاتها الصافية وحجمها الكبير يجعلها معيارًا قياسيًا لتدريب وتقييم أنظمة التعرف التلقائي على الكلام (#en_std([ASR])).
])

#linebreak()

=== #ar([تقسيم البيانات وإنقساماتها])

#ar_std([
  لضمان عملية تدريب وتقييم قوية، تم تقسيم مجموعة بيانات #en_std([LibriSpeech]) إلى مجموعات فرعية متميزة. نستخدم التقسيمات التالية:

  - *مجموعات التدريب والتحقق*: نستخدم القسم #en_std([`train-clean-100`])، الذي يحتوي على #en_std([$100$]) ساعة من الكلام الصافي والمقروء. يتم تقسيم هذه المجموعة بشكل إضافي لإنشاء بيانات التدريب والتحقق الخاصة بنا. يتم تخصيص #en_std([$80%$]) من البيانات لتدريب النموذج، بينما يتم الاحتفاظ بالـ #en_std([$20%$]) المتبقية كمجموعة تحقق لمراقبة الأداء ومنع #en_std([Overfitting]) أثناء عملية التدريب.
  - *مجموعة الاختبار*: للتقييم النهائي للنموذج المدرب، نستخدم تقسيم #en_std([`test-clean`]). تحتوي هذه المجموعة على بيانات لم يرها النموذج مطلقًا أثناء التدريب أو التحقق، مما يوفر قياسًا غير متحيز لأدائه في العالم الحقيقي.

  يتم تحميل جميع مجموعات البيانات على دفعات من #en_std([$32$]) عينة. يتم خلط بيانات التدريب في بداية كل حقبة لضمان عدم تعلم النموذج ترتيب الأمثلة.
])

#linebreak()

=== #ar([المعالجة المسبقة للبيانات])

#ar_std([
  قبل إدخالها في النموذج، تخضع البيانات الصوتية والنصية الخام لسلسلة من خطوات المعالجة المسبقة لتحويلها إلى صيغة مناسبة لتدريب الشبكة العصبية.
])

#pagebreak()

==== #ar([معالجة الصوت])

#ar_std([
  يتم تحويل أشكال الموجات الصوتية الخام إلى تمثيل غني بالميزات يمكن للنموذج التعلم منه. ويتم ذلك عن طريق تحويل كل ملف صوتي إلى *مخطط طيفي #en_std([Log-Mel])*. تتضمن هذه العملية عدة خطوات:

  1. يتم تحميل الصوت بمعدل عينة يبلغ #en_std([$16000$]) هرتز.
  2. يتم تطبيق تحويل فورييه قصير المدى (#en_std([STFT])) على إشارة الصوت لتحليل محتواها الترددي بمرور الوقت. نستخدم تحويل فورييه سريع (#en_std([FFT])) بحجم #en_std([$1024$]) وطول قفزة #en_std([$512$]).
  3. يتم تحويل الطيف الصوتي الناتج إلى مقياس #en_std([Mel])، الذي يقترب بشكل أفضل من الإدراك السمعي البشري. نستخدم #en_std([$100$]) صندوق #en_std([Mel]).
  4. أخيرًا، يتم تطبيق دالة لوغاريتمية على مقدار الطيف الصوتي #en_std([Mel]). هذا يضغط النطاق الديناميكي للميزات ويجعل النموذج أسهل في التدريب.
])

#linebreak()

==== #ar([معالجة النصوص])

#ar_std([
  كما تتم معالجة النصوص المقابلة لإنشاء أهداف رقمية للنموذج. يتم تحديد مفردات الأحرف بدقة لضمان التناسق.

  1. *المفردات*: تتكون مفردات النموذج من #en_std([$26$]) حرفًا إنجليزيًا صغيرًا، وعلامة اقتباس (#en_std([`'`]))، وحرف فراغ، وأربعة رموز خاصة: #en_std([`<pad>`])، و #en_std([`<unk>`])، و #en_std([`<sos>`])، و #en_std([`<eos>`]).
  2. *الترميز*: يتم تحويل كل النصوص إلى أحرف صغيرة. ثم يتم ترميز كل نص إلى سلسلة من الأحرف الفردية.
  3. *الرموز الخاصة*: يتم إرفاق الرمز #en_std([`<sos>`]) (بداية الجملة) في بداية كل تسلسل، ويتم إرفاق الرمز #en_std([`<eos>`]) (نهاية الجملة) في نهايته. يشير هذا إلى بداية ونهاية مهمة النسخ بالنسبة للمفكك.
  4. *تعيين الأعداد الصحيحة*: يتم تعيين كل حرف في التسلسل المُعين إلى رقم تعريفي صحيح فريد بناءً على المفردات المحددة مسبقًا. يتم تعيين أي حرف غير موجود في المفردات إلى الرمز الخاص #en_std([`<unk>`]) (غير معروف). يتم استخدام الرمز #en_std([`<pad>`]) لملء جميع التسلسلات في الدفعة بنفس الطول.
])

#pagebreak()

== #ar([بنية النموذج])

#ar_std([
  جوهر هذا المشروع هو تطبيق مخصص لنموذج #en_std([Listen, Attend, and Spell]) (#en_clean([LAS])) ، تم إنشاؤه باستخدام #en_std([TensorFlow]) و #en_std([Keras]). تتكون البنية من نموذجين فرعيين رئيسيين: #en_std([Listener]) (المشفّر) و #en_std([Speller]) (المهجئ) ، اللذان يعملان معًا لتحويل مخططات الطيف الصوتي إلى تسلسلات أحرف. يوضح هذا القسم بالتفصيل الطبقات والبارامترات المحددة لتطبيقنا.
])

#linebreak()

=== #ar([المستمع (المشفّر)])

#ar_std([
  دور المستمع هو معالجة مخطط الطيف الصوتي #en_std([Log-Mel]) للإدخال وإنشاء تمثيل عالي المستوى ومجرّد للميزات. وهو مصمم كشبكة هرمية عميقة لتقليل البعد الزمني للصوت المدخل بشكل فعال، مما يسهل على المتهجئ الانتباه إليه. الطبقات مرتبة على النحو التالي:

  1. *طبقة #en_std([LSTM]) ثنائية الاتجاه القاعية (#en_std([BLSTM]))*: يمر الطيف الصوتي المدخل أولاً عبر طبقة #en_std([BLSTM]) معيارية. تعالج هذه الطبقة التسلسل في كلا الاتجاهين الأمامي والخلفي، وتلتقط المعلومات السياقية من المقطع الصوتي بأكمله. تستخدم #en_std([$256$]) وحدة #en_std([LSTM]) لكل اتجاه.
  2. *طبقة التطبيع*: يتم تطبيع ناتج طبقة #en_std([BLSTM]) لتحقيق الاستقرار في عملية التدريب وتحسين تقارب النموذج.
  3. *مكدس #en_std([BLSTM]) الهرمي (#en_std([pBLSTM]))*: يتم بعد ذلك إدخال الناتج المطابق في مكدس مكون من ثلاث طبقات #en_std([pBLSTM]). تقلل كل طبقة #en_std([pBLSTM]) من الدقة الزمنية لتسلسل مدخلاتها بمعامل اثنين عن طريق ربط الخطوات الزمنية المتتالية. تخلق هذه البنية الهرمية تمثيلًا مضغوطًا وغنيًا بالمعلومات للصوت الأصلي. تستخدم كل طبقة #en_std([pBLSTM]) أيضًا #en_std([$256$]) وحدة #en_std([LSTM]) لكل اتجاه.

  الناتج النهائي للمستمع هو تسلسل من متجهات السمات #en_std([h]) التي تكون أقصر بثمانية أضعاف من الطيف الصوتي الأصلي.
])

#pagebreak()

=== #ar([المُهجّئ (جهاز فك التشفير)])

#ar_std([
  المُهجئ هو مُفكك يعتمد على الانتباه يأخذ ناتج المستمع #en_std([`h`]) ويُنشئ النص النهائي حرفًا حرفًا. تم تصميم بنيته للتنبؤ بالحرف التالي بناءً على سياق الصوت والحروف التي أنشأها سابقًا.

  1. *طبقة #en_std([Character Embedding])*: يأخذ المُهجّئ رقم #en_std([ID]) الصحيح للحرف الذي تم إنشاؤه مسبقًا كمدخلات ويحوله إلى تمثيل متجه كثيف باستخدام طبقة #en_std([embedding]). يتم تحديد بُعد #en_std([embedding]) بـ #en_std([$256$]).
  2. *مكدس #en_std([LSTM]) للمفكك*: يتم دمج #en_std([embedding]) الأحرف مع متجه السياق من الخطوة الزمنية السابقة ويتم إدخاله في مكدس من طبقتين #en_std([LSTM]) أحاديتي الاتجاه. تحافظ هذه الطبقات على الحالة الداخلية للمفكك، والتي تلخص التسلسل الذي تم إنشاؤه حتى الآن. تستخدم كل طبقة #en_std([LSTM]) #en_std([$512$]) وحدة وتشمل #en_std([dropout]) و #en_std([recurrent dropout]) للتنظيم.
  3. *الانتباه متعدد الرؤوس المدرك للموقع*: هذا هو المكون الأكثر أهمية في المُهجئ. في كل خطوة من خطوات فك التشفير، تحسب آلية الانتباه متجه السياق من خلال الانتباه إلى مخرجات المستمع. يستخدم تطبيقنا آلية *انتباه مدرك للموقع* متطورة مع *أربعة رؤوس*. وهذا يسمح للنموذج ليس فقط بالتركيز على الأجزاء ذات الصلة من الصوت (الوعي بالمحتوى) ولكن أيضًا بمعرفة الأجزاء التي انتبه إليها سابقًا (الوعي بالموقع)، مما يمنعه من التوقف أو تخطي أجزاء من الصوت. تستخدم آلية الانتباه بُعدًا داخليًا من #en_std([$512$]) وحدة.
  4. *طبقة توزيع الأحرف*: أخيرًا، يتم ربط ناتج وحدة فك التشفير #en_std([LSTM]) بمتجه السياق الجديد ويمر عبر طبقة #en_std([`Dense`]) نهائية. تنتج هذه الطبقة قيمًا أولية، والتي يتم تحويلها على الفور إلى توزيع احتمالي على كامل مفردات الأحرف باستخدام دالة تنشيط #en_std([*Softmax*]).
])

#pagebreak()

== #ar([التدريب])

#ar_std([
  كان تدريب النموذج عملية تكرارية ومتعددة المراحل مصممة لتحسين أدائه تدريجياً. يقدم هذا القسم أولاً موجزاً عن المنهجية العامة، بما في ذلك كل من #en_std([optimizer]) و #en_std([loss function]) والاستراتيجيات التي تم تطبيقها بشكل ثابت في جميع المراحل. ثم يقدم تفاصيل عن الإعدادات المحددة والنتائج لكل مرحلة تدريب على حدة.
])

#linebreak()

=== #ar([منهجية التدريب])

#ar_std([
  لضمان تعلم مستقر وفعال، تم استخدام مجموعة ثابتة من الأدوات والاستراتيجيات طوال عملية التدريب.
])

#linebreak()

==== #ar([#en_clean([Optimizer]) و #en_clean([Loss Function])])

#ar_std([
  تم تدريب النموذج باستخدام #en_std([*Adam Optimizer*])، وهو خيار شائع لمهام التعلم العميق نظرًا لكفاءته وقدراته على التعلم التكيفي. لمنع مشكلة #en_std([*exploding gradients*])، التي يمكن أن تؤدي إلى زعزعة استقرار التدريب، تم تطبيق معيار #en_std([*gradient clipping*]) على جميع تحديثات الأوزان.

  تم استخدام #en_std([loss function]) مخصص، #en_std([`safe_sparse_categorical_crossentropy`])، في هذا المشروع. عادةً ما تتوقع #en_std([cross-entropy loss]) الشائعة في #en_std([TensorFlow]) إدخال #en_std([logits]) الخامة. نظرًا لأن الطبقة النهائية لنموذجنا تتضمن تنشيط #en_std([Softmax])، فقد تم تصميم هذا التابع المخصص للعمل مباشرةً مع الاحتمالات الناتجة. ميزاته الرئيسية هي:
  1.  يحسب بشكل صحيح #en_std([cross-entropy loss]) من مخرجات احتمالية #en_std([softmax]).
  2.  تقوم تلقائيًا بتجاهل الرموز #en_std([`<pad>`])، مما يضمن أن الأجزاء المملوءة من التسلسلات المجمعة لا تساهم في حساب #en_std([loss]).
  3.  تتضمن عدة عمليات تصحيح أخطاء وفحوصات أمان لضمان الاستقرار العددي أثناء التدريب.
])

#pagebreak()

==== #ar([المقاييس والتقييم])

#ar_std([
  تمت مراقبة أداء النموذج طوال فترة التدريب باستخدام أربعة مقاييس رئيسية:
  - #en_std([*Loss*]): القيمة من #en_std([cross-entropy loss function]) المخصصة.
  - *الدقة*: الدقة لكل حرف.
  - *معدل خطأ الحرف (#en_std([CER]))*: النسبة المئوية للأحرف التي تم توقعها بشكل غير صحيح.
  - *معدل خطأ الكلمة (#en_std([WER]))*: المقياس القياسي لأنظمة #en_std([ASR])، الذي يقيس الأخطاء على مستوى الكلمة.
])

#linebreak()

==== #ar([استراتيجية #en_clean([Scheduled Sampling])])

#ar_std([
  استخدم التدريب استراتيجية مدارة بعناية لسد الفجوة بين التدريب والاستدلال. تضمن النهج العام عدة مراحل:

  1. *الإحماء التمهيدي*: تم استخدام فترة إحماء تمهيدية، تم خلالها الحفاظ على استقرار احتمالية أخذ العينات. سمح ذلك للنموذج بالاستقرار قبل تعرضه لتنبؤاته الخاصة، التي قد تكون مشوشة.
  2. *دورات التصعيد والاستقرار*: بعد الإحماء، استمر التدريب في دورات. تم زيادة احتمالية أخذ العينات تدريجياً على مدى عدد محدد من الحقبات، تلاها فترة استقرار حيث بقيت الاحتمالية ثابتة. سمحت هذه العملية التكرارية للنموذج بالتكيف مع الصعوبة المتزايدة دون أن يصبح غير مستقر.
])

#linebreak()

#ar_std([
  يتم تحديث الاحتمال، #en_std([$epsilon$])، في بداية كل حقبة خلال مرحلة التصعيد. يتم حساب الاحتمال الجديد للحقبة الحالية، #en_std([$epsilon_e$])، عن طريق إضافة زيادة خطية ثابتة إلى الاحتمال من الحقبة السابقة، #en_std([$epsilon_(e-1)$]).

  يمكن التعبير عن قاعدة التحديث هذه على النحو التالي:
])

#align(center)[
  #en_std([
    $epsilon_e = epsilon_(e-1) + (epsilon_f - epsilon_s) / E_r$
  ])
]

#linebreak()

#ar_std([
  حيث:
  - #en_std([$epsilon_e$]): احتمال أخذ العينات للحقبة الحالية #en_std([$e$]).
  - #en_std([$epsilon_(e-1)$]): احتمال أخذ العينات للحقبة السابقة.
  - #en_std([$epsilon_s$]): احتمال الابتدائي في بداية مرحلة التدرج.
  - #en_std([$epsilon_f$]): الاحتمال الأخير في نهاية مرحلة التدرج.
  - #en_std([$E_r$]): إجمالي عدد الحقبات التي يتم خلالها زيادة الاحتمال.
])

#linebreak()

#ar_std([
  ثم يتم قيد القيمة الناتجة لضمان عدم تجاوزها الاحتمال النهائي المستهدف، #en_std([$epsilon_f$]). كما أنها تتعامل بشكل صحيح مع الحالة التي يكون فيها #en_std([$epsilon_f < epsilon_s$]) وتقلل الاحتمال دون التسبب في أخطاء.
])

#linebreak()

=== #ar([المرحلة الأولى: التدريب الأولي])

#ar_std([
  ركزت المرحلة الأولى من التدريب، التي استمرت على مدار #en_std([$240$]) حقبة، على إنشاء أداء أساسي. خلال هذه المرحلة، تم تكوين البنية باستخدام آلية #en_std([*Multi-Head Attention*]) القياسية، بدون المكون الذي يدرك الموقع. كان الهدف الأساسي هو ضمان قدرة النموذج على تعلم المهمة الأساسية المتمثلة في تحويل الكلام إلى نص والعثور على مجموعة مستقرة من #en_std([Hyperparameters]).

  يوضح الجدول أدناه تفاصيل #en_std([Hyperparameters]) الرئيسية المستخدمة خلال هذه المرحلة.
])

#figure(
  table(
    columns: 3,
    align: (left+horizon, center+horizon, center+horizon),
    en_std[*Hyperparameter*], ar_std([*القيمة الأولية*]), ar_std([*القيمة النهائية*]),
    en_std([Learning Rate]), en_std([$0.001$]), en_std([$0.0003$]),
    en_std([Gradient Clipping Norm]), en_std([$5.0$]), en_std([$2.5$]),
    en_std([Sampling Probability (`epsilon`)]), en_std([$0.05$]), en_std([$0.5$]),
  ),
  kind: table,
  caption: flex_captions(
    ar_std([قيم كل من #en_clean_std([Learning Rate]) و #en_clean_std([Gradient Clipping Norm]) و #en_clean_std([Sampling Probability]) خلال المرحلة الأولى من التدريب.]),
    ar_std([قيم #en_clean_std([Hyperparameters]) في المرحلة الأولى])
  )
)

#pagebreak()

#ar_std([
  توضح الرسوم البيانية التالية تطور قيم #en_std([metrics]) على مدار 240 حقبة من التدريب في المرحلة الأولى:
])

#grid(
  columns: 2,
  gutter: 4pt,
  // Figure 1: Model Loss
  figure(
    image("../media/Training_Phase-1_Model-Loss.png", width: 100%),
    kind: image,
    caption: flex_captions(
      ar_std([قيم #en_clean_std([loss]) للنموذج على مدار #en_clean_std([$240$]) حقبة من تدريب المرحلة الأولى.]),
      ar_std([قيم #en_clean_std([loss]) للنموذج في المرحلة الأولى])
    )
  ),

  // Figure 2: Model Accuracy
  figure(
    image("../media/Training_Phase-1_Model-Accuracy.png", width: 100%),
    kind: image,
    caption: flex_captions(
      ar_std([دقة كل حرف خلال التدريب في المرحلة الأولى.]),
      ar_std([دقة النموذج في المرحلة الأولى])
    )
  ),

  // Figure 3: Character Error Rate (CER)
  figure(
    image("../media/Training_Phase-1_Model-CER.png", width: 100%),
    kind: image,
    caption: flex_captions(
      ar_std([معدل خطأ الحرف (#en_clean_std([CER])) خلال التدريب في المرحلة الأولى.]),
      ar_std([قيم #en_clean_std([CER]) في المرحلة الأولى])
    )
  ),

  // Figure 4: Word Error Rate (WER)
  figure(
    image("../media/Training_Phase-1_Model-WER.png", width: 100%),
    kind: image,
    caption: flex_captions(
      ar_std([معدل الخطأ في الكلمات (#en_clean_std([WER])) خلال التدريب في المرحلة الأولى.]),
      ar_std([قيم #en_clean_std([WER]) في المرحلة الأولى])
    )
  )
)

#pagebreak()

#ar_std([
  يوضح الجدول التالي أمثلة للنصوص من نقاط مختلفة خلال هذه المرحلة، مما يوضح تقدم النموذج في التعلم.
])

#figure(
  table(
    columns: 3,
    align: (center+horizon, center+horizon, center+horizon),
    inset: 8pt,
    ar_std([*حقبة*]), ar_std([*نوع الخرج*]), ar_std([*النص*]),

    table.cell(rowspan: 3, en_std([$15$])),
    en_std([True Output]), en_std[do we really know the mountain well when we are not acquainted with the cavern],
    en_std[Sampled Output], en_std[di we really know the mountain well when we are not acquainted with the cave n],
    en_std([Non-Sampled Output]), en_std([```text 'aaa  wweeeeddiinneeeddiinneeeddii  wwooddeer        aann          asssssss  aann             aann        asssssss  aann             hhettiinnsss        asssssss  aann             aann          hheee             aann        hhettiinnsss        asssssss  aann             aann   '```]),

    table.cell(rowspan: 3, en_std([$120$])),
    en_std([True Output]), en_std([do we really know the mountain well when we are not acquainted with the cavern]),
    en_std([Sampled Output]), en_std([do we really know thatmountain well ween we are not accuainted tith aaa cavern]),
    en_std([Non-Sampled Output]), en_std([```text 'tee  rrr                                                                       '```]),

    table.cell(rowspan: 3, en_std([$240$])),
    en_std([True Output]), en_std([do we really know the mountain well when we are not acquainted with the cavern]),
    en_std([Sampled Output]), en_std([do ye rellly koow the mountain well whon we are not a quainted mith the cavern]),
    en_std([Non-Sampled Output]), en_std([```text 'doo   rrellly  ooww tthe  nnn tto wwell wwill  werre nntttt   qquiint  ff miicc  ccaareen'```]),
  ),
  kind: table,
  caption: flex_captions(
    ar_std([مقارنة بين نتائج النموذج لنفس العينة الصوتية في مراحل زمنية مختلفة خلال المرحلة الأولى من التدريب.]),
    ar_std([مقارنة للنتائج في المرحلة الأولى])
  )
)

#linebreak()

#ar_std([
  تُظهر النتائج الواردة في الجدول بوضوح التحسينات المستمرة لنوع الخرج #en_std([non-sampled]) بمرور الوقت. في البداية كانت المخرجات عشوائية وغير مفهمومة، ولكن في النهاية بدأت أنماط تركيب كلمات منطقية مما يشير إلى أن النموذج بدأ يتعلم التعرف على الكلمات. من ناحية أخرى، تدهور المخرجات من نوع #en_std([sampled]) مع زيادة احتمال أخذ العينات لكنها ما تزال مفهومة.
])

#linebreak()
